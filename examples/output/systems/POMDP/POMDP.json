{"metadata":{"exportDate":"2025-03-10T17:35:42.074Z","entityCount":9,"relationshipCount":16},"entities":[{"id":"pomdp_model","type":"model","attributes":{"name":"Active Inference POMDP Model","description":"A Partially Observable Markov Decision Process implemented with Active Inference","domain":"computational_neuroscience","model_type":"discrete","discount_factor":0.95,"horizon":10,"time_steps":100}},{"id":"observation_space","type":"space","attributes":{"name":"Observation Space","symbol":"o","dimensions":[3,1],"description":"The set of possible observations an agent can perceive","observations":[{"id":"o1","name":"Observation 1","description":"Low sensory input"},{"id":"o2","name":"Observation 2","description":"Medium sensory input"},{"id":"o3","name":"Observation 3","description":"High sensory input"}]}},{"id":"state_space","type":"space","attributes":{"name":"Latent State Space","symbol":"s","dimensions":[5,1],"description":"The set of possible hidden states the environment can be in","states":[{"id":"s1","name":"State 1","description":"Very low state value"},{"id":"s2","name":"State 2","description":"Low state value"},{"id":"s3","name":"State 3","description":"Medium state value"},{"id":"s4","name":"State 4","description":"High state value"},{"id":"s5","name":"State 5","description":"Very high state value"}]}},{"id":"action_space","type":"space","attributes":{"name":"Action Space","symbol":"a","dimensions":[2,1],"description":"The set of possible actions an agent can take","actions":[{"id":"a1","name":"Action 1","description":"Decrease value"},{"id":"a2","name":"Action 2","description":"Increase value"}]}},{"id":"a_matrix","type":"matrix","attributes":{"name":"Observation Likelihood Matrix","symbol":"A","dimensions":[3,5],"description":"Maps hidden states to observations (likelihood mapping)","matrix_type":"likelihood","matrix_data":[[0.8,0.3,0.1,0,0],[0.2,0.6,0.7,0.3,0.1],[0,0.1,0.2,0.7,0.9]],"precision":0.9,"column_labels":["s1","s2","s3","s4","s5"],"row_labels":["o1","o2","o3"]}},{"id":"b_matrix","type":"matrix","attributes":{"name":"State Transition Matrix","symbol":"B","dimensions":[5,5,2],"description":"Defines the dynamics of state transitions based on actions","matrix_type":"transition","matrix_data":[[[0.9,0.2,0,0,0],[0.1,0.7,0.2,0,0],[0,0.1,0.7,0.2,0],[0,0,0.1,0.7,0.1],[0,0,0,0.1,0.9]],[[0.9,0.1,0,0,0],[0.1,0.7,0.1,0,0],[0,0.2,0.7,0.1,0],[0,0,0.2,0.7,0.1],[0,0,0,0.2,0.9]]],"action_labels":["a1","a2"],"column_labels":["s1","s2","s3","s4","s5"],"row_labels":["s1","s2","s3","s4","s5"]}},{"id":"c_matrix","type":"matrix","attributes":{"name":"Preference Matrix","symbol":"C","dimensions":[3,1],"description":"Specifies the agent preferences over observations (as log probabilities)","matrix_type":"preference","matrix_data":[[-3],[0],[3]],"row_labels":["o1","o2","o3"],"precision":1}},{"id":"d_matrix","type":"matrix","attributes":{"name":"Initial State Prior","symbol":"D","dimensions":[5,1],"description":"Defines the prior distribution over initial states","matrix_type":"prior","matrix_data":[[0.2],[0.2],[0.4],[0.1],[0.1]],"row_labels":["s1","s2","s3","s4","s5"]}},{"id":"e_matrix","type":"matrix","attributes":{"name":"Policy Prior","symbol":"E","dimensions":[2,1],"description":"Defines the prior distribution over policies/actions","matrix_type":"policy_prior","matrix_data":[[0.5],[0.5]],"row_labels":["a1","a2"]}}],"relationships":[{"source":"pomdp_model","target":"observation_space","type":"includes","attributes":{}},{"source":"pomdp_model","target":"state_space","type":"includes","attributes":{}},{"source":"pomdp_model","target":"action_space","type":"includes","attributes":{}},{"source":"pomdp_model","target":"a_matrix","type":"includes","attributes":{}},{"source":"pomdp_model","target":"b_matrix","type":"includes","attributes":{}},{"source":"pomdp_model","target":"c_matrix","type":"includes","attributes":{}},{"source":"pomdp_model","target":"d_matrix","type":"includes","attributes":{}},{"source":"pomdp_model","target":"e_matrix","type":"includes","attributes":{}},{"source":"a_matrix","target":"observation_space","type":"maps_from","attributes":{}},{"source":"a_matrix","target":"state_space","type":"maps_to","attributes":{}},{"source":"b_matrix","target":"state_space","type":"maps_from","attributes":{}},{"source":"b_matrix","target":"state_space","type":"maps_to","attributes":{}},{"source":"b_matrix","target":"action_space","type":"conditioned_on","attributes":{}},{"source":"c_matrix","target":"observation_space","type":"evaluates","attributes":{}},{"source":"d_matrix","target":"state_space","type":"distributes_over","attributes":{}},{"source":"e_matrix","target":"action_space","type":"distributes_over","attributes":{}}]}